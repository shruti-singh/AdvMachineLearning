{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-06 21:03:45--  http://www.gutenberg.org/files/31100/31100.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4454075 (4.2M) [text/plain]\n",
      "Saving to: ‘31100.txt’\n",
      "\n",
      "31100.txt           100%[===================>]   4.25M   566KB/s    in 12s     \n",
      "\n",
      "2020-10-06 21:03:59 (358 KB/s) - ‘31100.txt’ saved [4454075/4454075]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File available in the code repo\n",
    "\n",
    "# # Wget the works of Jane Austen\n",
    "# !wget http://www.gutenberg.org/files/31100/31100.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"31100.txt\", \"r\") as f:\n",
    "    text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/shruti/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function SeekableUnicodeStreamReader.__del__ at 0x7f07c5fda830>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shruti/.local/lib/python3.7/site-packages/nltk/data.py\", line 1161, in __del__\n",
      "    if not self.closed:\n",
      "  File \"/home/shruti/.local/lib/python3.7/site-packages/nltk/data.py\", line 1175, in closed\n",
      "    return self.stream.closed\n",
      "AttributeError: 'SeekableUnicodeStreamReader' object has no attribute 'stream'\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15354"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = X.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = dict(zip(vectorizer.get_feature_names(),counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_word_dict = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 28621),\n",
       " ('to', 26199),\n",
       " ('and', 24238),\n",
       " ('of', 23163),\n",
       " ('her', 13929),\n",
       " ('in', 12278),\n",
       " ('was', 11762),\n",
       " ('it', 10856),\n",
       " ('she', 10619),\n",
       " ('not', 9085)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_word_dict[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain top 100 most frequently occurring words after removing the stopwords\n",
    "\n",
    "vocab_dict = defaultdict(int)\n",
    "\n",
    "counter = 0\n",
    "for tup in sorted_word_dict:\n",
    "    if counter == 100:\n",
    "        break\n",
    "    if not tup[0] in stopwords.words():\n",
    "        vocab_dict[tup[0]] = tup[1]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_id_map = defaultdict(int)\n",
    "id_to_vocab_map = defaultdict(int)\n",
    "\n",
    "for idx, i in enumerate(list(vocab_dict.keys())):\n",
    "    vocab_to_id_map[i] = idx\n",
    "    id_to_vocab_map[idx] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,\n",
       " ['could',\n",
       "  'would',\n",
       "  'mr',\n",
       "  'mrs',\n",
       "  'must',\n",
       "  'said',\n",
       "  'much',\n",
       "  'miss',\n",
       "  'well',\n",
       "  'every'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dict), list(vocab_dict.keys())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "v = 100\n",
    "d = 2000\n",
    "\n",
    "alpha = 0.01\n",
    "beta = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Ids:  [26, 99, 84, 34, 50, 47, 94, 65, 74, 35, 18, 70, 82, 67, 18, 57, 11, 32, 90, 86]\n",
      "lady, yes, love, quite, elizabeth, sure, really, friend, crawford, made, though, jane, yet, enough, though, many, never, sir, letter, brother, "
     ]
    }
   ],
   "source": [
    "topic_ids = list(np.random.randint(0, 101, 20))\n",
    "print(\"Topic Ids: \", topic_ids)\n",
    "\n",
    "for i in topic_ids:\n",
    "    print(id_to_vocab_map[i], end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_id_map = defaultdict(int)\n",
    "id_to_topic_map = defaultdict(int)\n",
    "\n",
    "for idx, i in enumerate(topic_ids):\n",
    "    topic_to_id_map[i] = idx\n",
    "    id_to_topic_map[idx] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distributions():\n",
    "    fs = np.random.uniform(0, 1, 20)\n",
    "    tup = []\n",
    "    for i in fs:\n",
    "        tup.append(int(i*100))\n",
    "    topic_distribution = np.random.dirichlet(tup, 2000)\n",
    "\n",
    "    fs = np.random.uniform(0, 1, 20)\n",
    "    tup = []\n",
    "    for i in fs:\n",
    "        tup.append(int(i*100))\n",
    "    word_distribution = np.random.dirichlet(tup, 100)\n",
    "    \n",
    "    return topic_distribution, word_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distribution, word_distribution = generate_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 20), (100, 20))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_distribution.shape, word_distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_doc():\n",
    "    random_idx = np.random.randint(0, 2000)\n",
    "    cumulative_topic_probs_for_idx = defaultdict(float)\n",
    "\n",
    "    for idx, i in enumerate(topic_distribution[random_idx]):\n",
    "        if idx > 0:\n",
    "            cumulative_topic_probs_for_idx[idx] = cumulative_topic_probs_for_idx[idx-1] + i\n",
    "        else:\n",
    "            cumulative_topic_probs_for_idx[idx] = i\n",
    "\n",
    "    # print(topic_distribution[random_idx])\n",
    "    # print(cumulative_topic_probs_for_idx)\n",
    "\n",
    "    # Selecting a topic for the document\n",
    "    select_topic_prob = np.random.uniform(0, 1)\n",
    "\n",
    "    selected_topic_id_for_doc_gen = None\n",
    "    for i in range(0, 19):\n",
    "        if select_topic_prob > cumulative_topic_probs_for_idx[i] and select_topic_prob < cumulative_topic_probs_for_idx[i+1]:\n",
    "            selected_topic_id_for_doc_gen = i\n",
    "\n",
    "    if not selected_topic_id_for_doc_gen:\n",
    "        selected_topic_id_for_doc_gen = 19\n",
    "\n",
    "\n",
    "    # Calculate the cumulative prob for the words\n",
    "    cumulative_word_probs_for_idx = defaultdict(float)\n",
    "    for idx, i in enumerate(word_distribution[selected_topic_id_for_doc_gen]):\n",
    "        if idx > 0:\n",
    "            cumulative_word_probs_for_idx[idx] = cumulative_word_probs_for_idx[idx-1] + i\n",
    "        else:\n",
    "            cumulative_word_probs_for_idx[idx] = i\n",
    "\n",
    "    document_words = []\n",
    "    # Generate 100 words for the document\n",
    "    for w in range(0, 100):\n",
    "        # Selecting a word prob\n",
    "        select_word_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        selected_word_id_for_topic = None\n",
    "        for i in range(0, 99):\n",
    "            if select_word_prob > cumulative_word_probs_for_idx[i] and select_word_prob < cumulative_word_probs_for_idx[i+1]:\n",
    "                selected_word_id_for_topic = i\n",
    "\n",
    "        if not selected_word_id_for_topic:\n",
    "            selected_word_id_for_topic = 99\n",
    "        document_words.append(selected_word_id_for_topic)\n",
    "    \n",
    "    return random_idx, document_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1252 [1, 2, 15, 13, 8, 6, 15, 8, 15, 8, 7, 8, 4, 13, 8, 2, 16, 7, 9, 16, 11, 1, 16, 11, 6, 1, 14, 11, 9, 16, 14, 1, 4, 3, 13, 99, 10, 13, 15, 6, 15, 1, 15, 1, 8, 14, 1, 2, 13, 13, 11, 13, 9, 3, 6, 2, 16, 10, 8, 8, 16, 1, 10, 8, 11, 99, 15, 16, 10, 13, 16, 13, 1, 10, 11, 8, 13, 16, 16, 8, 1, 17, 15, 1, 16, 15, 8, 10, 8, 99, 6, 14, 99, 6, 8, 9, 16, 6, 6, 13]\n"
     ]
    }
   ],
   "source": [
    "did, doc_words = generate_doc()\n",
    "print(did, doc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['would', 'mr', 'time', 'good', 'well', 'much', 'time', 'well', 'time', 'well', 'miss', 'well', 'must', 'good', 'well', 'mr', 'little', 'miss', 'every', 'little', 'never', 'would', 'little', 'never', 'much', 'would', 'might', 'never', 'every', 'little', 'might', 'would', 'must', 'mrs', 'good', 'yes', 'think', 'good', 'time', 'much', 'time', 'would', 'time', 'would', 'well', 'might', 'would', 'mr', 'good', 'good', 'never', 'good', 'every', 'mrs', 'much', 'mr', 'little', 'think', 'well', 'well', 'little', 'would', 'think', 'well', 'never', 'yes', 'time', 'little', 'think', 'good', 'little', 'good', 'would', 'think', 'never', 'well', 'good', 'little', 'little', 'well', 'would', 'nothing', 'time', 'would', 'little', 'time', 'well', 'think', 'well', 'yes', 'much', 'might', 'yes', 'much', 'well', 'every', 'little', 'much', 'much', 'good']\n"
     ]
    }
   ],
   "source": [
    "document_text = []\n",
    "for i in doc_words:\n",
    "    document_text.append(id_to_vocab_map[i])\n",
    "\n",
    "print(document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood of the generated text:  -307.53754115389484\n"
     ]
    }
   ],
   "source": [
    "prob_of_doc = 0\n",
    "log_wi_given_d = []\n",
    "\n",
    "for wi in doc_words:\n",
    "    wi_given_d = 0\n",
    "    for ti in range(0, 20):\n",
    "        wi_given_d += word_distribution[wi][ti]*topic_distribution[did][ti]\n",
    "    log_wi_given_d.append(np.log(wi_given_d))\n",
    "print(\"Log likelihood of the generated text: \", np.sum(log_wi_given_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1711 [1, 15, 8, 8, 11, 15, 6, 13, 10, 4, 15, 14, 13, 15, 15, 13, 15, 13, 13, 12, 13, 2, 16, 15, 99, 15, 1, 6, 13, 2, 15, 15, 9, 99, 4, 11, 17, 12, 4, 12, 9, 14, 1, 13, 9, 17, 6, 12, 16, 1, 16, 13, 99, 5, 13, 99, 11, 6, 12, 99, 15, 17, 17, 13, 1, 13, 99, 99, 6, 8, 13, 13, 8, 4, 17, 8, 8, 2, 14, 13, 2, 1, 13, 13, 4, 16, 1, 8, 16, 8, 12, 8, 4, 17, 8, 16, 3, 2, 99, 13]\n",
      "Log likelihood of the generated text:  -304.30582649510853\n"
     ]
    }
   ],
   "source": [
    "did, doc_words = generate_doc()\n",
    "print(did, doc_words)\n",
    "\n",
    "prob_of_doc = 0\n",
    "log_wi_given_d = []\n",
    "\n",
    "for wi in doc_words:\n",
    "    wi_given_d = 0\n",
    "    for ti in range(0, 20):\n",
    "        wi_given_d += word_distribution[wi][ti]*topic_distribution[did][ti]\n",
    "    log_wi_given_d.append(np.log(wi_given_d))\n",
    "print(\"Log likelihood of the generated text: \", np.sum(log_wi_given_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
